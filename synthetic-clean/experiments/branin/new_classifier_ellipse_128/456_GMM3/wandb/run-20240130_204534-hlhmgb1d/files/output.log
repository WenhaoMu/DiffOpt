('out_directory: '
 './experiments/branin/new_classifier_ellipse_128/456_GMM3/wandb/run-20240130_204534-hlhmgb1d/files')
shapesss (6040, 2) (1, 6040, 1)
shapesss (6040, 2) (6040, 1)
[8.40824839e-21 9.66923258e-20 1.11254525e-18 1.28087196e-17
 1.47931871e-16 1.70184561e-15 1.95155433e-14 2.24314893e-13
 2.57221530e-12 2.95893976e-11 3.40580609e-10 3.86106714e-09
 4.40428979e-08 4.98776984e-07 5.61496433e-06 6.32810815e-05
 7.38673042e-04 8.06684931e-03 9.10087297e-02 9.00116169e-01]
/nethome/wmu30/anaconda3/envs/design-baseliness/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:53: LightningDeprecationWarning: Setting `max_steps = None` is deprecated in v1.5 and will no longer be supported in v1.7. Use `max_steps = -1` instead.
  "Setting `max_steps = None` is deprecated in v1.5 and will no longer be supported in v1.7."
/nethome/wmu30/anaconda3/envs/design-baseliness/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:97: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=20)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.
  f"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and"
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
Sanity Checking DataLoader 0:   0%|                                                                                       | 0/2 [00:00<?, ?it/s]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]
  | Name    | Type                  | Params
--------------------------------------------------
0 | mlp     | MLP                   | 2.1 M
1 | inf_sde | VariancePreservingSDE | 1
--------------------------------------------------
2.1 M     Trainable params
1         Non-trainable params
2.1 M     Total params
Epoch 0:  83%|███████████████████████████████████████████████▌         | 40/48 [00:01<00:00, 38.92it/s, loss=0.72, v_num=gb1d, train_loss=0.637]
Validation: 0it [00:00, ?it/s]
/nethome/wmu30/anaconda3/envs/design-baseliness/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:1937: PossibleUserWarning: The number of training batches (43) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.


















Epoch 24:  83%|████████████████████████████████▌      | 40/48 [00:01<00:00, 32.33it/s, loss=0.581, v_num=gb1d, train_loss=0.419, val_loss=0.540]















Epoch 41: 100%|███████████████████████████████████████| 48/48 [00:01<00:00, 24.75it/s, loss=0.598, v_num=gb1d, train_loss=0.575, val_loss=0.525]











Epoch 53:  83%|████████████████████████████████▌      | 40/48 [00:01<00:00, 34.42it/s, loss=0.576, v_num=gb1d, train_loss=0.449, val_loss=0.542]










Epoch 67:  83%|████████████████████████████████▌      | 40/48 [00:01<00:00, 33.76it/s, loss=0.537, v_num=gb1d, train_loss=0.564, val_loss=0.596]











Epoch 81:  83%|████████████████████████████████▌      | 40/48 [00:01<00:00, 29.63it/s, loss=0.539, v_num=gb1d, train_loss=0.438, val_loss=0.597]















Epoch 97:  83%|████████████████████████████████▌      | 40/48 [00:01<00:00, 32.38it/s, loss=0.545, v_num=gb1d, train_loss=0.589, val_loss=0.531]























Epoch 125:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 36.31it/s, loss=0.547, v_num=gb1d, train_loss=0.526, val_loss=0.588]


































Epoch 170:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 31.39it/s, loss=0.555, v_num=gb1d, train_loss=0.599, val_loss=0.587]




























Epoch 204:  83%|████████████████████████████████▌      | 40/48 [00:01<00:00, 36.12it/s, loss=0.56, v_num=gb1d, train_loss=0.524, val_loss=0.546]







Epoch 225:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 35.24it/s, loss=0.569, v_num=gb1d, train_loss=0.638, val_loss=0.555]

Epoch 227:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 34.37it/s, loss=0.557, v_num=gb1d, train_loss=0.584, val_loss=0.521]
















Epoch 243:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 30.76it/s, loss=0.535, v_num=gb1d, train_loss=0.560, val_loss=0.526]


























Epoch 284:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 33.88it/s, loss=0.573, v_num=gb1d, train_loss=0.438, val_loss=0.495]

























Epoch 325:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 33.80it/s, loss=0.547, v_num=gb1d, train_loss=0.472, val_loss=0.535]























Epoch 360:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 33.23it/s, loss=0.577, v_num=gb1d, train_loss=0.651, val_loss=0.562]












Epoch 376:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 35.25it/s, loss=0.548, v_num=gb1d, train_loss=0.463, val_loss=0.507]








Epoch 389:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 31.46it/s, loss=0.562, v_num=gb1d, train_loss=0.631, val_loss=0.532]













Epoch 406:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 34.49it/s, loss=0.562, v_num=gb1d, train_loss=0.481, val_loss=0.562]












Epoch 422:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 31.14it/s, loss=0.547, v_num=gb1d, train_loss=0.634, val_loss=0.553]


Epoch 433:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 36.49it/s, loss=0.544, v_num=gb1d, train_loss=0.429, val_loss=0.570]

Epoch 435:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 34.72it/s, loss=0.559, v_num=gb1d, train_loss=0.603, val_loss=0.518]




Epoch 440:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 38.53it/s, loss=0.549, v_num=gb1d, train_loss=0.512, val_loss=0.522]















Epoch 456:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 32.85it/s, loss=0.557, v_num=gb1d, train_loss=0.570, val_loss=0.571]

Epoch 467:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 34.61it/s, loss=0.577, v_num=gb1d, train_loss=0.631, val_loss=0.580]

Epoch 472:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 35.63it/s, loss=0.547, v_num=gb1d, train_loss=0.529, val_loss=0.603]









































Epoch 513:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 31.66it/s, loss=0.562, v_num=gb1d, train_loss=0.533, val_loss=0.564]





Epoch 531:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 38.42it/s, loss=0.553, v_num=gb1d, train_loss=0.459, val_loss=0.486]






















Epoch 562:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 36.68it/s, loss=0.572, v_num=gb1d, train_loss=0.628, val_loss=0.495]











Epoch 574:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 32.96it/s, loss=0.544, v_num=gb1d, train_loss=0.521, val_loss=0.492]
















Epoch 613:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 23.37it/s, loss=0.581, v_num=gb1d, train_loss=0.584, val_loss=0.528]

Epoch 615:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 35.09it/s, loss=0.596, v_num=gb1d, train_loss=0.469, val_loss=0.571]
































Epoch 648:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 34.57it/s, loss=0.563, v_num=gb1d, train_loss=0.551, val_loss=0.544]
















Epoch 670:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 29.04it/s, loss=0.573, v_num=gb1d, train_loss=0.412, val_loss=0.552]







Epoch 678:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 30.50it/s, loss=0.533, v_num=gb1d, train_loss=0.526, val_loss=0.564]




Epoch 683:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 28.24it/s, loss=0.553, v_num=gb1d, train_loss=0.475, val_loss=0.565]







Epoch 691:  83%|████████████████████████████████▌      | 40/48 [00:01<00:00, 31.97it/s, loss=0.58, v_num=gb1d, train_loss=0.588, val_loss=0.526]







Epoch 703:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 33.10it/s, loss=0.562, v_num=gb1d, train_loss=0.547, val_loss=0.569]









Epoch 714:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 31.00it/s, loss=0.542, v_num=gb1d, train_loss=0.561, val_loss=0.597]





Epoch 722:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 30.27it/s, loss=0.551, v_num=gb1d, train_loss=0.404, val_loss=0.552]








Epoch 732:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 30.52it/s, loss=0.556, v_num=gb1d, train_loss=0.546, val_loss=0.533]









Epoch 743:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 31.02it/s, loss=0.544, v_num=gb1d, train_loss=0.508, val_loss=0.507]








Epoch 753:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 32.10it/s, loss=0.569, v_num=gb1d, train_loss=0.517, val_loss=0.530]










Epoch 765:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 30.70it/s, loss=0.575, v_num=gb1d, train_loss=0.461, val_loss=0.522]











Epoch 777:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 33.07it/s, loss=0.544, v_num=gb1d, train_loss=0.547, val_loss=0.578]













Epoch 793:  83%|████████████████████████████████▌      | 40/48 [00:01<00:00, 31.83it/s, loss=0.54, v_num=gb1d, train_loss=0.413, val_loss=0.531]









Epoch 806:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 32.01it/s, loss=0.566, v_num=gb1d, train_loss=0.492, val_loss=0.514]













Epoch 822:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 34.57it/s, loss=0.581, v_num=gb1d, train_loss=0.534, val_loss=0.525]













Epoch 839:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 34.61it/s, loss=0.526, v_num=gb1d, train_loss=0.457, val_loss=0.526]











































Epoch 885:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 34.45it/s, loss=0.557, v_num=gb1d, train_loss=0.520, val_loss=0.577]




















Epoch 913:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 32.82it/s, loss=0.546, v_num=gb1d, train_loss=0.461, val_loss=0.509]










Epoch 925:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 31.35it/s, loss=0.568, v_num=gb1d, train_loss=0.620, val_loss=0.599]









Epoch 936:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 32.84it/s, loss=0.551, v_num=gb1d, train_loss=0.614, val_loss=0.602]










Epoch 950:  83%|████████████████████████████████▌      | 40/48 [00:01<00:00, 31.62it/s, loss=0.56, v_num=gb1d, train_loss=0.565, val_loss=0.604]










Epoch 963:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 32.21it/s, loss=0.557, v_num=gb1d, train_loss=0.511, val_loss=0.551]




Epoch 970:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 32.32it/s, loss=0.565, v_num=gb1d, train_loss=0.601, val_loss=0.443]







Epoch 978:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 31.90it/s, loss=0.564, v_num=gb1d, train_loss=0.506, val_loss=0.574]








Epoch 988:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 30.40it/s, loss=0.546, v_num=gb1d, train_loss=0.455, val_loss=0.589]








Epoch 997:  83%|███████████████████████████████▋      | 40/48 [00:01<00:00, 30.69it/s, loss=0.531, v_num=gb1d, train_loss=0.511, val_loss=0.524]

Validation: 0it [00:00, ?it/s]