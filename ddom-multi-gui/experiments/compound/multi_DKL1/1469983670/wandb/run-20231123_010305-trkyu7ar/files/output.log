('out_directory: '
 './experiments/compound/multi_DKL1/1469983670/wandb/run-20231123_010305-trkyu7ar/files')
The device used is : cuda
[0.00316918 0.0072529  0.01241131 0.01619215 0.01927074 0.02262353
 0.02631122 0.03051457 0.03533518 0.04085936 0.04722409 0.05451293
 0.06284052 0.07218581 0.08231845 0.09331833 0.10262941 0.10205275
 0.09619775 0.07277971]
DEVICE: cuda
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
/nethome/wmu30/anaconda3/envs/design-baseliness/lib/python3.7/site-packages/gpytorch/lazy/triangular_lazy_tensor.py:136: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.
torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.
X = torch.triangular_solve(B, A).solution
should be replaced with
X = torch.linalg.solve_triangular(A, B). (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2115.)
  res = torch.triangular_solve(right_tensor, self.evaluate(), upper=self.upper).solution
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
OUTPUT SHAPE:  torch.Size([128])
Traceback (most recent call last):
  File "design_baselines/diff_multi/DKL_train_regression_x0_multi.py", line 713, in <module>
    device=device,
  File "design_baselines/diff_multi/DKL_train_regression_x0_multi.py", line 434, in run_classifier
    loss.backward()
  File "/nethome/wmu30/anaconda3/envs/design-baseliness/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/nethome/wmu30/anaconda3/envs/design-baseliness/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt